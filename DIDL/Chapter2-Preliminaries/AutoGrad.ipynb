{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "953baa84-3457-44c1-9304-58c1fdb334aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 12:04:06.638121: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1157591-4439-46ef-9283-d28e003c05ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.range(4, dtype=tf.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91d46ea9-c5bb-4070-8664-43cf93688524",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "641a4a9d-38f4-478e-98db-a0720c49fd96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=28.0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Record all computations onto a tape\n",
    "with tf.GradientTape() as t:\n",
    "    y = 2 * tf.tensordot(x, x, axes=1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07306387-e111-49de-9533-adefe9cac560",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.,  4.,  8., 12.], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_grad = t.gradient(y, x)\n",
    "x_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b86e6880-86d7-4537-963d-e7b413314689",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    y = tf.reduce_sum(x)\n",
    "t.gradient(y, x)  # Overwritten by the newly calculated gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "256f6e4a-e9c7-41ce-8f4a-17c52886f053",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Trying a simpler example\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "#### Trying a simpler example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71a553cb-39e3-4bdb-984b-cdbd618c5891",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 3.,  7., 11., 15.], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    y = (2 * (x ** 2)) + (3 * x) + 10\n",
    "t.gradient(y, x)  # Overwritten by the newly calculated gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3b460e1-b424-4c59-97f8-ca4f45467d96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Detaching Computation\n",
       "\n",
       "Sometimes, we wish to move some calculations outside of the recorded computational graph.\n",
       "For example, say that we use the input to create some auxiliary intermediate terms for which\n",
       "we do not want to compute a gradient. In this case, we need to detach the respective computational\n",
       "graph from the final result. The following toy example makes this clearer:\n",
       "suppose we have z = x * y and y = x + x but we want to focus on the direct influence of x on z\n",
       "rather than the influence conveyed via y. In this case, we can create a new variable u that takes\n",
       "the same value as y but whose provenance (how it was created) has been wiped out. Thus u has no\n",
       "ancestors in the graph and gradients do not flow through u to x. For example, taking the gradient\n",
       "of z = x * u will yield the result u, (not 3 * x * x as you might have expected since z = x * x * x).\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "### Detaching Computation\n",
    "\n",
    "Sometimes, we wish to move some calculations outside of the recorded computational graph.\n",
    "For example, say that we use the input to create some auxiliary intermediate terms for which\n",
    "we do not want to compute a gradient. In this case, we need to detach the respective computational\n",
    "graph from the final result. The following toy example makes this clearer:\n",
    "suppose we have z = x * y and y = x + x but we want to focus on the direct influence of x on z\n",
    "rather than the influence conveyed via y. In this case, we can create a new variable u that takes\n",
    "the same value as y but whose provenance (how it was created) has been wiped out. Thus u has no\n",
    "ancestors in the graph and gradients do not flow through u to x. For example, taking the gradient\n",
    "of z = x * u will yield the result u, (not 3 * x * x as you might have expected since z = x * x * x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3c9bbc20-55f8-42fc-bb82-93f3654fdd68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.,  2., 16., 54.], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set persistent=True to preserve the compute graph.\n",
    "# This lets us run t.gradient more than once\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    y = x ** 3\n",
    "    u = tf.stop_gradient(y)\n",
    "    z = 2 * u * x\n",
    "\n",
    "x_grad = t.gradient(z, x)\n",
    "x_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "387aa763-637e-4712-a38e-558a715d5faa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_grad == 2 * u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "23cb3f8c-7fdf-4f6c-b6e4-d0d4633174fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.,  3., 12., 27.], dtype=float32)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_grad_based_on_x = t.gradient(y, x)\n",
    "y_grad_based_on_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c3269cf6-2b0c-469c-8a18-9f5779585f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_grad_based_on_x == 3 * (x ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "02944674-a9f6-4536-aee2-daf19c4f8034",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Gradients and Python Control Flow\n",
       "\n",
       "So far we reviewed cases where the path from input to output was well defined via a function such as z = x * x * x. Programming offers us a lot more freedom in how we compute results. For instance, we can make them depend on auxiliary variables or condition choices on intermediate results. One benefit of using automatic differentiation is that even if building the computational graph of a function required passing through a maze of Python control flow (e.g., conditionals, loops, and arbitrary function calls), we can still calculate the gradient of the resulting variable. To illustrate this, consider the following code snippet where the number of iterations of the while loop and the evaluation of the if statement both depend on the value of the input a.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "### Gradients and Python Control Flow\n",
    "\n",
    "So far we reviewed cases where the path from input to output was well defined via a function such as z = x * x * x. Programming offers us a lot more freedom in how we compute results. For instance, we can make them depend on auxiliary variables or condition choices on intermediate results. One benefit of using automatic differentiation is that even if building the computational graph of a function required passing through a maze of Python control flow (e.g., conditionals, loops, and arbitrary function calls), we can still calculate the gradient of the resulting variable. To illustrate this, consider the following code snippet where the number of iterations of the while loop and the evaluation of the if statement both depend on the value of the input a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a5f1a55c-827e-4971-95b4-9d8bcf5f8d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while tf.norm(b) < 1000:\n",
    "        b = b * 2\n",
    "    if tf.reduce_sum(b) > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "240e7c98-ebe6-46b8-9139-2b11cb4b962e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below, we call this function, passing in a random value, as input. Since the input is a random variable, we do not know what form the computational graph will take. However, whenever we execute f(a) on a specific input, we realize a specific computational graph and can subsequently run backward.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "Below, we call this function, passing in a random value, as input. Since the input is a random variable, we do not know what form the computational graph will take. However, whenever we execute f(a) on a specific input, we realize a specific computational graph and can subsequently run backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2a62bcf1-a201-48a5-aa09-4d61faecb79f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1024.0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.Variable(tf.random.normal(shape=()))\n",
    "with tf.GradientTape() as t:\n",
    "    d = f(a)\n",
    "d_grad = t.gradient(d, a)\n",
    "d_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
